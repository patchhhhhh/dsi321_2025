import requests
import pandas as pd
from datetime import datetime, timedelta
import os

# ğŸ”‘ API Key à¸‚à¸­à¸‡ NewsAPI
API_KEY = "your_newsapi_key_here"  # â† à¹ƒà¸ªà¹ˆà¸‚à¸­à¸‡à¸„à¸¸à¸“à¹€à¸­à¸‡à¸—à¸µà¹ˆà¸™à¸µà¹ˆ

# ğŸ” à¸„à¸³à¸„à¹‰à¸™à¸«à¸²
QUERIES = [
    "Life Cycle Assessment of Alternative Construction Materials",
    "Physical and Mechanical Properties of Compressed Earth Blocks",
    "The Potential of Bamboo as a Structural Construction Material",
    "Applications of Concrete Mixed with Recycled Materials",
    "Thermal Insulation from Natural Materials",
    "Developing Construction Materials from Agricultural Waste",
    "Bio-based Construction Materials",
    "The Use of 3D Printing with Alternative Construction Materials",
    "Designing Energy-Efficient Buildings with Alternative Construction Materials",
    "Standards and Regulations for Alternative Construction Materials",
    "Adoption and Acceptance of Alternative Construction Materials in Thailand",
    "The Economic Impact of Using Alternative Construction Materials on Local Communities",
    "Promoting Alternative Construction Materials for Sustainable Development",
    "Consumer Awareness and Understanding of Alternative Construction Materials",
    "Opportunities and Challenges in the Production of Alternative Construction Materials in Thailand",
    "Developing Alternative Construction Materials with Enhanced Properties",
    "Improving the Properties of Alternative Construction Materials with New Technologies",
    "Research and Development of Alternative Construction Materials from Local Resources",
    "Integrating Alternative Construction Materials into the Circular Economy Concept",
    "Building Networks and Collaboration for the Development of Alternative Construction Materials"
]

CSV_FILENAME = "construction_materials_newsapi.csv"
NEWSAPI_ENDPOINT = "https://newsapi.org/v2/everything"

# ğŸ“… à¸§à¸±à¸™à¸¢à¹‰à¸­à¸™à¸«à¸¥à¸±à¸‡ 5 à¸›à¸µ
five_years_ago = datetime.now() - timedelta(days=5*365)
from_date = five_years_ago.strftime('%Y-%m-%d')
to_date = datetime.now().strftime('%Y-%m-%d')

# ğŸ“‚ à¹‚à¸«à¸¥à¸” URL à¸—à¸µà¹ˆà¹€à¸„à¸¢à¸šà¸±à¸™à¸—à¸¶à¸
def load_seen_urls():
    if os.path.exists(CSV_FILENAME):
        df = pd.read_csv(CSV_FILENAME)
        return set(df['url'].values)
    return set()

# ğŸ” à¸”à¸¶à¸‡à¸‚à¹ˆà¸²à¸§à¸ˆà¸²à¸ NewsAPI
def search_news(query, page_size=100, max_pages=10):
    seen_urls = load_seen_urls()
    all_data = []

    for page in range(1, max_pages + 1):
        params = {
            "q": query,
            "from": from_date,
            "to": to_date,
            "sortBy": "relevancy",
            "language": "en",
            "apiKey": API_KEY,
            "pageSize": page_size,
            "page": page
        }

        response = requests.get(NEWSAPI_ENDPOINT, params=params)
        result = response.json()

        if result.get("status") != "ok":
            print(f"âŒ Error fetching data for '{query}':", result.get("message"))
            break

        articles = result.get("articles", [])
        if not articles:
            break

        for article in articles:
            url = article["url"]
            if url in seen_urls:
                continue
            seen_urls.add(url)

            all_data.append({
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "query": query,
                "title": article["title"],
                "url": url,
                "publishedAt": article["publishedAt"],
                "source": article["source"]["name"],
                "description": article.get("description", ""),
                "content": article.get("content", "")
            })

        print(f"âœ… Fetched {len(articles)} results for '{query}' (page {page})")

    return all_data

# ğŸ’¾ à¸šà¸±à¸™à¸—à¸¶à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¥à¸‡ CSV
def save_to_csv(data, filename=CSV_FILENAME):
    df_new = pd.DataFrame(data)

    if os.path.exists(filename):
        df_old = pd.read_csv(filename)
        df_all = pd.concat([df_old, df_new], ignore_index=True)
    else:
        df_all = df_new

    df_all.to_csv(filename, index=False)
    print(f"ğŸ’¾ Saved {len(data)} rows to '{filename}'\n")

# ğŸ“¥ à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸à¸—à¸¸à¸ query
all_results = []
results_needed = 1000
queries_handled = 0

# à¸„à¸³à¸™à¸§à¸“à¸§à¹ˆà¸²à¹€à¸£à¸²à¸ˆà¸°à¸•à¹‰à¸­à¸‡à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸à¸«à¸¥à¸²à¸¢à¸«à¸™à¹‰à¸²à¹à¸¥à¸°à¸«à¸¥à¸²à¸¢à¸„à¸³à¸„à¹‰à¸™
for query in QUERIES:
    if queries_handled >= results_needed:
        break
    data = search_news(query, page_size=100, max_pages=10)  # à¹ƒà¸Šà¹‰à¸«à¸™à¹‰à¸² 10 à¹€à¸à¸·à¹ˆà¸­à¸”à¸¶à¸‡ 100 à¸£à¸²à¸¢à¸à¸²à¸£à¸•à¹ˆà¸­à¸«à¸™à¹‰à¸²
    all_results.extend(data)
    queries_handled += len(data)

# ğŸ’¾ à¸šà¸±à¸™à¸—à¸¶à¸
if all_results:
    save_to_csv(all_results)
else:
    print("âš ï¸ No data fetched.")
