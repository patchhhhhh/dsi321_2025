from serpapi import GoogleSearch
import pandas as pd
from datetime import datetime, timedelta
import os

# üîë API Key ‡∏à‡∏≤‡∏Å serpapi.com
API_KEY = "15179a882a5e319597d7dcb0fa7a2e61516f3cfbca11839352ad2f5545f0b5aa"  # ‡πÉ‡∏™‡πà API Key ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà

# üîç ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡∏±‡∏™‡∏î‡∏∏‡∏Å‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô
QUERIES = [
    "Life Cycle Assessment of Alternative Construction Materials",
    "Physical and Mechanical Properties of Compressed Earth Blocks",
    "The Potential of Bamboo as a Structural Construction Material",
    "Applications of Concrete Mixed with Recycled Materials",
    "Thermal Insulation from Natural Materials",
    "Developing Construction Materials from Agricultural Waste",
    "Bio-based Construction Materials",
    "The Use of 3D Printing with Alternative Construction Materials",
    "Designing Energy-Efficient Buildings with Alternative Construction Materials",
    "Standards and Regulations for Alternative Construction Materials",
    "Adoption and Acceptance of Alternative Construction Materials in Thailand",
    "The Economic Impact of Using Alternative Construction Materials on Local Communities",
    "Promoting Alternative Construction Materials for Sustainable Development",
    "Consumer Awareness and Understanding of Alternative Construction Materials",
    "Opportunities and Challenges in the Production of Alternative Construction Materials in Thailand",
    "Developing Alternative Construction Materials with Enhanced Properties",
    "Improving the Properties of Alternative Construction Materials with New Technologies",
    "Research and Development of Alternative Construction Materials from Local Resources",
    "Integrating Alternative Construction Materials into the Circular Economy Concept",
    "Building Networks and Collaboration for the Development of Alternative Construction Materials"
]

CSV_FILENAME = "construction_materials_serpapi.csv"

# üìÖ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 5 ‡∏õ‡∏µ
five_years_ago = datetime.now() - timedelta(days=5*365)

# üóÇ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ú‡πà‡∏≤‡∏ô GoogleSearch
def search_google(query, num_pages=3):
    all_data = []
    seen_urls = set()  # ‡πÉ‡∏ä‡πâ‡πÄ‡∏ã‡πá‡∏ï‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö URL ‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏à‡∏≠

    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡πÄ‡∏Å‡πà‡∏≤ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    if os.path.exists(CSV_FILENAME):
        df_existing = pd.read_csv(CSV_FILENAME)
        seen_urls.update(df_existing['url'].values)  # ‡∏ô‡∏≥ URL ‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏à‡∏≠‡πÅ‡∏•‡πâ‡∏ß‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV

    for page in range(num_pages):
        params = {
            "q": query,
            "api_key": API_KEY,
            "num": 10,  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤
            "start": page * 10  # ‡∏Ç‡πâ‡∏≤‡∏°‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏õ
        }

        search = GoogleSearch(params)
        results = search.get_dict()
        organic = results.get("organic_results", [])

        data = []
        for item in organic:
            url = item.get("link")
            title = item.get("title")
            snippet = item.get("snippet", "")
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ URL ‡∏´‡∏£‡∏∑‡∏≠ title ‡∏ã‡πâ‡∏≥
            if url not in seen_urls and title not in seen_urls:
                seen_urls.add(url)  # ‡πÄ‡∏û‡∏¥‡πà‡∏° URL ‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏õ‡πÉ‡∏ô‡πÄ‡∏ã‡πá‡∏ï

                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡∏±‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏ß‡πà‡∏≤‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ä‡πà‡∏ß‡∏á 5 ‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                try:
                    # ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡∏´‡∏≤‡∏Å Google Search ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏≤
                    # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢‡∏î‡∏π‡∏à‡∏≤‡∏Å‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ô timestamp
                    # (‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• timestamp ‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÅ‡∏ó‡∏ô)
                    year_in_snippet = int(snippet[-4:])  # ‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥‡∏ß‡πà‡∏≤ snippet ‡∏°‡∏µ‡∏õ‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡∏ó‡πâ‡∏≤‡∏¢‡∏™‡∏∏‡∏î
                    if year_in_snippet >= five_years_ago.year:
                        data.append({
                            "timestamp": timestamp,
                            "query": query,
                            "title": title,
                            "url": url,
                            "snippet": snippet
                        })
                except Exception as e:
                    # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏µ‡πÉ‡∏ô snippet ‡∏Å‡πá‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 5 ‡∏õ‡∏µ
                    data.append({
                        "timestamp": timestamp,
                        "query": query,
                        "title": title,
                        "url": url,
                        "snippet": snippet
                    })

        print(f"‚úÖ Fetched {len(data)} results for: '{query}' (page {page + 1})")
        all_data.extend(data)

    return all_data

# üíæ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á CSV
def save_to_csv(data, filename=CSV_FILENAME):
    df_new = pd.DataFrame(data)

    if os.path.exists(filename):
        df_old = pd.read_csv(filename)
        df_all = pd.concat([df_old, df_new], ignore_index=True)
    else:
        df_all = df_new

    df_all.to_csv(filename, index=False)
    print(f"üíæ Saved {len(data)} rows to '{filename}'\n")

# üì• ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å query
all_results = []
for query in QUERIES:
    data = search_google(query, num_pages=5)  # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô 5 ‡∏´‡∏ô‡πâ‡∏≤
    all_results.extend(data)

# üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á CSV
if all_results:
    save_to_csv(all_results)
else:
    print("‚ö†Ô∏è No data fetched.")
